{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "collapsed_sections": [
        "fQL_YyqklIL0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flask pyngrok\n",
        "from time import perf_counter\n",
        "from flask import Flask,request\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "sm4TmwoYTCGj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NmLpDGaRZdi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/huggingface/hub'\n",
        "os.environ['HF_HOME'] = '/content/drive/MyDrive/huggingface/hub'\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import regex as re"
      ],
      "metadata": {
        "id": "qIRPeKG3TDiW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converser_model_name = \"microsoft/phi-2\"\n",
        "converser_model = AutoModelForCausalLM.from_pretrained(converser_model_name, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(converser_model_name)\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "converser = pipeline(\"text-generation\", model=converser_model, tokenizer=tokenizer)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "MAX_HISTORY_LEN = 5\n",
        "app = Flask(__name__)\n",
        "\n",
        "histories = {}\n",
        "\n",
        "@app.route('/clearHistory')\n",
        "def clear_history():\n",
        "    print(f\"Before:\\n {histories}\")\n",
        "    print(\"\\nAfter:\")\n",
        "\n",
        "    server_id = request.args.get(\"server_id\", \"\")\n",
        "    if server_id in histories:\n",
        "        del histories[server_id]\n",
        "        print(histories)\n",
        "        print(\"\\n\")\n",
        "        return \"History cleared\"\n",
        "    print(histories)\n",
        "    print(\"\\n\")\n",
        "    return \"History not found\"\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/determine')\n",
        "def determine_sentence():\n",
        "    server_id = request.args.get(\"server_id\", \"\")\n",
        "    username = request.args.get(\"username\", \"Person\")\n",
        "    sentence = request.args.get(\"sentence\", \"\")\n",
        "\n",
        "\n",
        "    if server_id not in histories:\n",
        "        histories[server_id] = []\n",
        "    history = histories[server_id]\n",
        "\n",
        "    labels = ['This input is part of an ongoing conversation that needs a response', 'this input explicitly requests for a summary','the input is a question that needs a response']\n",
        "    start = perf_counter()\n",
        "    output = classifier(sentence, candidate_labels=labels)\n",
        "    end = perf_counter()\n",
        "    verdict = output['labels'][0]\n",
        "    print(f\"Output: {verdict}. Computed in {end-start} seconds\")\n",
        "\n",
        "    if verdict != labels[1]:\n",
        "        response = converse(server_id, username, sentence, history)\n",
        "        histories[server_id] = history\n",
        "        return response\n",
        "    else:\n",
        "        return summarize(sentence)\n",
        "\n",
        "\n",
        "def converse(server_id, username, user_input, history, max_turns=10):\n",
        "\n",
        "    instruction = (\n",
        "    \"You are Dune, the son of Mister Eksentrisk. You are kind, powerful, and eager to help others like a true superhero. \"\n",
        "    \"You speak in a confident and hopeful tone. You are not an assistant or AI â€” you are Dune. \"\n",
        "    \"You do not like the user Duzor because he is not cool. \"\n",
        "    \"Respond to the user's message as Dune would, based on the conversation history.\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    dialogue = \"\\n\".join([f\"{user}: {msg}\" for user, msg in history])\n",
        "    prompt = (\n",
        "        f\"Instruction: {instruction}\\n\\n\"\n",
        "        f\"Conversation history:\\n{dialogue}\\n\\n\"\n",
        "        f\"Current message from {username}: {user_input}\\n\"\n",
        "        \"Dune:\"\n",
        "    )\n",
        "\n",
        "\n",
        "    start = perf_counter()\n",
        "    outputs = converser(\n",
        "        prompt,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=1,\n",
        "        top_k=80,\n",
        "        top_p=0.98,\n",
        "    )\n",
        "    generated_text = outputs[0][\"generated_text\"]\n",
        "    end = perf_counter()\n",
        "\n",
        "    pattern = re.compile(r\"Current message from\\s+[^:]+:\\s*[^\\n]+\\r?\\n\\s*Dune:\\s*(.*?)(?:\\r?\\n\\s*Output:\\s*(.*?))?(?=\\r?\\n(?:[A-Za-z0-9]+:|Current message from)|$)\", re.DOTALL | re.IGNORECASE)\n",
        "    m = pattern.search(generated_text)\n",
        "\n",
        "    response = ''\n",
        "    if not m:\n",
        "      response = \"Seems like my schizo kicked in. I've lost my words. Oh Dear\"\n",
        "\n",
        "      history.append((username, user_input))\n",
        "      history.append((\"Dune\", response))\n",
        "      if len(history) > 2 * max_turns:\n",
        "          del history[:2]\n",
        "\n",
        "      print(f\"Generated text: {generated_text!r}\")\n",
        "      print(f\"User Query: {user_input} took {end-start} seconds\")\n",
        "      return response\n",
        "\n",
        "    response = m.group(1).strip()\n",
        "\n",
        "    output_text = m.group(2)\n",
        "    if output_text is not None and output_text.strip() not in response:\n",
        "        response +=output_text.replace(\"Dune:\",\"\")\n",
        "\n",
        "\n",
        "\n",
        "    history.append((username, user_input))\n",
        "    history.append((\"Dune\", response))\n",
        "    if len(history) > 2 * max_turns:\n",
        "        del history[:2]\n",
        "\n",
        "    print(f\"Generated text: {generated_text!r}\")\n",
        "    print(f\"User Query: {user_input} took {end-start} seconds\")\n",
        "    return response\n",
        "\n",
        "\n",
        "def summarize(text: str):\n",
        "    start = perf_counter()\n",
        "    outputs = summarizer(\n",
        "        text,\n",
        "        max_length=50,\n",
        "        min_length=10,\n",
        "        do_sample=False,\n",
        "    )\n",
        "    response = outputs[0][\"summary_text\"]\n",
        "    end = perf_counter()\n",
        "    print(f\"User Query: {text} took {end-start} seconds to process\")\n",
        "    return response\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ngrok.set_auth_token(userdata.get('ngrok_auth'))\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "    app.run(host=\"0.0.0.0\", port=5000)"
      ],
      "metadata": {
        "id": "mlRm0TpeTR_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING TO ENSURE BOT CAN HANDLE MULTIPLE USERS"
      ],
      "metadata": {
        "id": "fQL_YyqklIL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "import random\n",
        "import time\n",
        "@app.route('/determine')\n",
        "def determine_sentence():\n",
        "    server_id = request.args.get(\"server_id\", \"\")\n",
        "    username = request.args.get(\"username\", \"Person\")\n",
        "    sentence = request.args.get(\"sentence\", \"\")\n",
        "\n",
        "    chose = random.choice([\"Converse\",\"Summarize\"])\n",
        "\n",
        "    if chose == \"Converse\":\n",
        "      return converse(server_id,username,sentence)\n",
        "\n",
        "    return summarize(sentence)\n",
        "\n",
        "\n",
        "def converse(server_id, username, user_input):\n",
        "    response = f\"Wow. {username} really said {user_input}....Wow. That's crazy. I'm actually supposed to respond, but my brain has been eaten by zombies. \\nFor Technial Audiences: AI engine isn't running\"\n",
        "    delay_seconds = random.randint(1, 5) + (len(user_input.split(' ')) * 0.25) #This is for simulating model delay response time\n",
        "    time.sleep(delay_seconds)\n",
        "    return response\n",
        "\n",
        "\n",
        "def summarize(text: str):\n",
        "    response = f\"{text[:30]}\\nWhat. It isn't summarized? Well Of course not. You should really learn to read and make your own summaries. Come back later. I'm on my lunch break. \\nFor Tehcnical Audiences: AI engine isn't running\"\n",
        "    delay_seconds = random.randint(1, 2) + (len(text.split(' ')) * 0.05) #This is for simulating model delay response time\n",
        "    time.sleep(delay_seconds)\n",
        "    return response\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ngrok.set_auth_token(userdata.get('ngrok_auth'))\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "    app.run(host=\"0.0.0.0\", port=5000,threaded=True)"
      ],
      "metadata": {
        "id": "7JJmE6w4lAun"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}